diff --git a/core/validator.py b/core/validator.py
index 3dfd5fa..498633b 100644
--- a/core/validator.py
+++ b/core/validator.py
@@ -13,8 +13,8 @@ from typing import Dict, List, Any, Tuple, Optional, Set
 import Levenshtein as lev
 
 from core.session import (
-    get_current_session,
-    update_session_status,
+    get_current_session,
+    update_session_status,
     get_session_dir
 )
 from core.importer import get_table_data, get_column_info
@@ -30,7 +30,7 @@ SCHEMAS_DIR = "schemas"
 def validate_data() -> Dict[str, Any]:
     """
     Validate the data in the current session and determine the best document type.
-
+
     Returns:
         Dict containing validation results
     """
@@ -38,17 +38,17 @@ def validate_data() -> Dict[str, Any]:
     session_hash = get_current_session()
     if not session_hash:
         raise ValueError("No active session found")
-
+
     # Get imported data
     table_name, columns, data = get_table_data(session_hash)
     column_info = get_column_info(session_hash)
-
+
     # Load available schemas
     schemas = load_schemas()
-
+
     if not schemas:
         raise ValueError("No schemas found in schemas directory")
-
+
     # Check for existing mapping file
     existing_mapping = None
     try:
@@ -57,7 +57,7 @@ def validate_data() -> Dict[str, Any]:
         logger.info(f"Found existing mapping with {len(existing_mapping)} entries")
     except Exception as e:
         logger.warning(f"Error loading existing mapping: {e}")
-
+
     if existing_mapping and len(existing_mapping) > 0:
         # Use existing mapping - determine which schema it's for
         # Look for any mapping that indicates a schema type
@@ -71,19 +71,19 @@ def validate_data() -> Dict[str, Any]:
                     break
             if schema_name:
                 break
-
+
         if not schema_name:
             # Couldn't determine schema from mapping, use default
             schema_name = next(iter(schemas))
             logger.warning(f"Couldn't determine schema from mapping, using {schema_name}")
-
+
         # Use the determined schema
         best_schema = schema_name
-
+
         # Convert existing_mapping to field_matches format
         best_matches = {}
         schema_fields = schemas[best_schema].get("schema", {})
-
+
+
         # For each field in the schema, find its mapped column
         for field_name, field_def in schema_fields.items():
             # Find which column maps to this field
@@ -92,12 +92,12 @@ def validate_data() -> Dict[str, Any]:
                 if type_info.get("type") == field_name:
                     mapped_column = column
                     break
-
+
             # If found, create a match entry
             if mapped_column:
                 # Get actual row data for validation
                 table_name, columns, data = get_table_data(session_hash)     
-
+
                 # Create a match object with required fields
                 best_matches[field_name] = {
                     "field": field_name,
@@ -120,7 +120,7 @@ def validate_data() -> Dict[str, Any]:
                         "invalid_samples": []
                     }
                 }
-
+
         # Calculate a match score based on the mapping coverage
         mapped_types = set(item.get("type", "") for item in existing_mapping.values())
         schema_types = set(schema_fields.keys())
@@ -129,37 +129,60 @@ def validate_data() -> Dict[str, Any]:
             best_score = max(coverage_pct, 75.0)  # Minimum 75% score for manual mappings
         else:
             best_score = 90.0  # Default if no schema fields
-
+
         logger.info(f"Using existing mapping for schema: {best_schema} with calculated score {best_score:.2f}%")
     else:
         # No existing mapping, match data against each schema
         schema_scores = {}
         schema_matches = {}
-
+
         for schema_name, schema in schemas.items():
             score, matches = match_schema(schema, column_info, data)
             schema_scores[schema_name] = score
             schema_matches[schema_name] = matches
             logger.info(f"Schema {schema_name} match score: {score:.2f}%")   
-
+
         # Find best matching schema
         best_schema = max(schema_scores, key=schema_scores.get)
         best_score = schema_scores[best_schema]
         best_matches = schema_matches[best_schema]
-
+
         logger.info(f"Best matching schema: {best_schema} with score {best_score:.2f}%")
-
+
+    # Get all columns from the database
+    _, all_columns, _ = get_table_data(session_hash)
+
+    # Create a dictionary to track which columns are mapped
+    mapped_columns = set(match.get("column") for match in best_matches.values() if match.get("column"))
+    unmapped_columns = set(all_columns) - mapped_columns
+
+    # Add unmapped columns to the field_matches with a special "unmapped" type
+    for col in unmapped_columns:
+        best_matches[f"UNMAPPED_{col}"] = {
+            "field": f"UNMAPPED_{col}",
+            "column": col,
+            "match_type": "unmapped",
+            "score": 0.0,
+            "validation": {
+                "valid": True,  # Consider unmapped columns as valid
+                "valid_count": 0,
+                "total_count": 0,
+                "valid_percentage": 0,
+                "errors": []
+            }
+        }
+
     # Validate all rows against the best schema
     row_validations = []
     for i, row in enumerate(data):
         row_validation = validate_row(row, schemas[best_schema], best_matches)
         row_validation["row_id"] = i + 1
         row_validations.append(row_validation)
-
+
     # Calculate overall validation stats
     valid_rows = sum(1 for r in row_validations if r["valid"])
     invalid_rows = len(row_validations) - valid_rows
-
+
     validation_results = {
         "schema_name": best_schema,
         "document_type": schemas[best_schema].get("type", best_schema),      
@@ -167,18 +190,20 @@ def validate_data() -> Dict[str, Any]:
         "total_rows": len(row_validations),
         "valid_rows": valid_rows,
         "invalid_rows": invalid_rows,
-        "success_rate": (valid_rows / len(row_validations) * 100) if row_validations else 0
+        "success_rate": (valid_rows / len(row_validations) * 100) if row_validations else 0,
+        "all_columns": all_columns,  # Include all columns in the validation results
+        "unmapped_columns": list(unmapped_columns)  # Include list of unmapped columns
     }
-
+
     # Update session status with document type
     document_type = schemas[best_schema].get("type", best_schema)
     update_session_status(
-        session_hash,
+        session_hash,
         file_path="",  # We don't need to update the file path
         document_type=document_type,
         operation="VALIDATE_DATA"
     )
-
+
     # Log validation results
     html_logger = HTMLLogger(session_hash)
     html_logger.log_validation(
@@ -187,12 +212,14 @@ def validate_data() -> Dict[str, Any]:
         field_matches=best_matches,
         row_validations=row_validations
     )
-
+
     return {
         "validation_results": validation_results,
         "field_matches": best_matches,
         "schema": schemas[best_schema],
-        "row_validations": row_validations
+        "row_validations": row_validations,
+        "all_columns": all_columns,  # Include all columns in the API response
+        "unmapped_columns": list(unmapped_columns)  # Include list of unmapped columns
     }


@@ -202,205 +229,302 @@ def load_schemas() -> Dict[str, Dict[str, Any]]:      
     """
     Load all schema definitions from the schemas directory.
     Uses caching to avoid repeated disk reads.
-
+
     Returns:
         Dict mapping schema names to schema objects
     """
     global _SCHEMA_CACHE
-
+
     # Return cached schemas if available
     if _SCHEMA_CACHE:
         return _SCHEMA_CACHE
-
+
     schemas = {}
-
+
     if not os.path.isdir(SCHEMAS_DIR):
         logger.warning(f"Schemas directory not found: {SCHEMAS_DIR}")        
         return schemas
-
+
     for filename in os.listdir(SCHEMAS_DIR):
         if filename.endswith(".json"):
             schema_path = os.path.join(SCHEMAS_DIR, filename)
             schema_name = os.path.splitext(filename)[0]
-
+
             try:
                 with open(schema_path, 'r') as f:
                     schema = json.load(f)
-
+
                 # Validate schema structure
                 if "schema" not in schema:
                     logger.warning(f"Invalid schema format in {schema_path}: 'schema' key not found")
                     continue
-
+
                 schemas[schema_name] = schema
                 logger.debug(f"Loaded schema: {schema_name}")
-
+
             except (json.JSONDecodeError, OSError) as e:
                 logger.error(f"Error loading schema {schema_path}: {e}")     
-
+
     # Cache for future use
     _SCHEMA_CACHE = schemas
     return schemas


-def match_schema(schema: Dict[str, Any], column_info: Dict[str, Dict[str, Any]],
+def match_schema(schema: Dict[str, Any], column_info: Dict[str, Dict[str, Any]],
                 data: List[Dict[str, Any]]) -> Tuple[float, Dict[str, Any]]: 
     """
     Match data columns against a schema to determine the compatibility score.
-
+
     Args:
         schema: Schema definition
         column_info: Information about data columns
         data: Imported data rows
-
+
     Returns:
         Tuple of (match_score, field_matches)
     """
     if "schema" not in schema:
         logger.error(f"Invalid schema format: 'schema' key not found")       
         return 0.0, {}
-
+
     schema_fields = schema["schema"]
     total_fields = len(schema_fields)
     matches = {}
-
-    # Track matched fields
+
+    # Track matched fields and columns
     matched_count = 0
     field_matches = {}
-
-    # For each schema field, find the best matching column
+    matched_columns = set()  # Keep track of columns that have been matched  
+
+    # First pass: Process fields with max_matches=1 (default) to ensure they get priority
     for field_name, field_def in schema_fields.items():
-        best_match = find_best_column_match(field_name, field_def, column_info, data, schema)
-
-        if best_match["score"] > 0:
+        # Skip fields with max_matches > 1, they'll be processed in the second pass
+        if field_def.get("max_matches", 1) > 1:
+            continue
+
+        best_match = find_best_column_match(field_name, field_def, column_info, data, schema, matched_columns)
+
+        if best_match["score"] > 0 and best_match["column"]:
             matched_count += 1
-
+            matched_columns.add(best_match["column"])  # Mark this column as matched
+
         field_matches[field_name] = best_match
-
+
+    # Second pass: Process fields with max_matches > 1
+    for field_name, field_def in schema_fields.items():
+        max_matches = field_def.get("max_matches", 1)
+        if max_matches <= 1:
+            continue  # Already processed in first pass
+
+        # For fields that can have multiple matches, find the best N matches 
+        matches_found = 0
+
+        # The primary match is already in field_matches if it was found      
+        if field_name in field_matches and field_matches[field_name]["score"] > 0:
+            matches_found = 1
+        else:
+            # Find the first match
+            best_match = find_best_column_match(field_name, field_def, column_info, data, schema, matched_columns)
+
+            if best_match["score"] > 0 and best_match["column"]:
+                matched_count += 1
+                matched_columns.add(best_match["column"])  # Mark this column as matched
+                matches_found = 1
+
+            field_matches[field_name] = best_match
+
+        # Find additional matches up to max_matches
+        while matches_found < max_matches:
+            # Create a new field name for the additional match
+            additional_field_name = f"{field_name}_{matches_found + 1}"      
+
+            # Find the next best match, excluding already matched columns    
+            additional_match = find_best_column_match(field_name, field_def, column_info, data, schema, matched_columns)
+
+            if additional_match["score"] > 0 and additional_match["column"]: 
+                matched_count += 1
+                matched_columns.add(additional_match["column"])  # Mark this column as matched
+                matches_found += 1
+
+                # Store the additional match with a numbered field name      
+                field_matches[additional_field_name] = additional_match      
+            else:
+                # No more good matches found
+                break
+
     # Calculate match score as percentage of matched fields
     match_score = (matched_count / total_fields * 100) if total_fields > 0 else 0
-
+
     return match_score, field_matches


-def find_best_column_match(field_name: str, field_def: Dict[str, Any],       
-                          column_info: Dict[str, Dict[str, Any]],
+def find_best_column_match(field_name: str, field_def: Dict[str, Any],       
+                          column_info: Dict[str, Dict[str, Any]],
                           data: List[Dict[str, Any]],
-                          schema: Dict[str, Any]) -> Dict[str, Any]:
+                          schema: Dict[str, Any],
+                          matched_columns: Optional[Set[str]] = None) -> Dict[str, Any]:
     """
     Find the best matching column for a schema field.
-
+
     Args:
         field_name: Name of the schema field
         field_def: Field definition from schema
         column_info: Information about data columns
         data: Imported data rows
         schema: Full schema object
-
+        matched_columns: Set of column names that have already been matched  
+
     Returns:
         Dict with best match information
     """
+    # Initialize matched_columns if not provided
+    if matched_columns is None:
+        matched_columns = set()
+
     # Get field slugs (possible column names)
     slugs = field_def.get("slug", [field_name])
-
-    # Try exact matches on the field_name first (e.g., "COMPANY_NAME" column matches "COMPANY_NAME" field)
-    if field_name in column_info:
+
+    # CONTENT-BASED MATCHING: Validate each column's content against this field type
+    best_content_match = None
+    best_content_score = 0
+
+    for col_name in column_info:
+        # Skip columns that have already been matched to other fields        
+        if col_name in matched_columns:
+            continue
+
+        # Validate this column's values against the field definition
+        validation = validate_field_values(col_name, field_def, data, schema)
+        validation_score = validation.get("valid_percentage", 0)
+
+        # If this is the best content match so far
+        if validation_score > best_content_score and validation_score > 50:  # Minimum threshold
+            best_content_score = validation_score
+            best_content_match = {
+                "field": field_name,
+                "column": col_name,
+                "match_type": "content_validation",
+                "score": validation_score,
+                "validation": validation
+            }
+
+    # If we found a good content match, return it
+    if best_content_match and best_content_score > 70:  # Higher threshold for confidence
+        return best_content_match
+
+    # FALLBACK TO NAME-BASED MATCHING if content matching didn't find a good match
+
+    # Try exact matches on the field_name first
+    if field_name in column_info and field_name not in matched_columns:      
+        validation = validate_field_values(field_name, field_def, data, schema)
         return {
             "field": field_name,
             "column": field_name,
             "match_type": "exact_field_name",
-            "score": 100.0,
-            "validation": validate_field_values(field_name, field_def, data, schema)
+            "score": 90.0,  # Lower than content matching
+            "validation": validation
         }
-
+
     # Try exact slug matches
     for slug in slugs:
-        if slug in column_info:
+        if slug in column_info and slug not in matched_columns:
+            validation = validate_field_values(slug, field_def, data, schema)
             return {
                 "field": field_name,
                 "column": slug,
                 "match_type": "exact_slug",
-                "score": 100.0,
-                "validation": validate_field_values(slug, field_def, data, schema)
+                "score": 85.0,  # Lower than exact field name
+                "validation": validation
             }
-
+
     # Try case-insensitive matching on field name
     field_name_lower = field_name.lower()
     for col_name in column_info:
-        if col_name.lower() == field_name_lower:
+        if col_name.lower() == field_name_lower and col_name not in matched_columns:
+            validation = validate_field_values(col_name, field_def, data, schema)
             return {
                 "field": field_name,
                 "column": col_name,
                 "match_type": "case_insensitive_field",
-                "score": 98.0,
-                "validation": validate_field_values(col_name, field_def, data, schema)
+                "score": 80.0,  # Lower than exact matches
+                "validation": validation
             }
-
+
     # Try case-insensitive slug matches
     slug_lower = [s.lower() for s in slugs]
     for col_name in column_info:
-        if col_name.lower() in slug_lower:
-            match_index = slug_lower.index(col_name.lower())
+        if col_name.lower() in slug_lower and col_name not in matched_columns
:
+            validation = validate_field_values(col_name, field_def, data, schema)
             return {
                 "field": field_name,
                 "column": col_name,
                 "match_type": "case_insensitive",
-                "score": 95.0,
-                "validation": validate_field_values(col_name, field_def, data, schema)
+                "score": 75.0,  # Lower than case-insensitive field name     
+                "validation": validation
             }
-
+
     # Try comparing with underscores/spaces removed and case insensitive     
     field_name_normalized = field_name.lower().replace("_", "").replace(" ", "")
     for col_name in column_info:
+        if col_name in matched_columns:
+            continue
+
         col_normalized = col_name.lower().replace("_", "").replace(" ", "")  
         if col_normalized == field_name_normalized:
+            validation = validate_field_values(col_name, field_def, data, schema)
             return {
                 "field": field_name,
                 "column": col_name,
                 "match_type": "normalized_match",
-                "score": 90.0,
-                "validation": validate_field_values(col_name, field_def, data, schema)
+                "score": 70.0,  # Lower than case-insensitive matches        
+                "validation": validation
             }
-
+
     # Try fuzzy matching with Levenshtein distance
     best_match = None
     best_score = 0
-
-    for col_name, col_info in column_info.items():
-        # Skip columns that are already matched to other fields with higher confidence
-
+
+    for col_name in column_info:
+        # Skip columns that have already been matched to other fields        
+        if col_name in matched_columns:
+            continue
+
         # Calculate similarity to each slug
         for slug in slugs:
             # Normalize strings for comparison
             slug_norm = slug.lower().replace("_", " ")
             col_norm = col_name.lower().replace("_", " ")
-
+
             # Calculate Levenshtein ratio (0-100)
             similarity = lev.ratio(slug_norm, col_norm) * 100
-
+
             # If this is the best match so far
             if similarity > best_score and similarity > 60:  # Minimum threshold
                 validation = validate_field_values(col_name, field_def, data, schema)
                 validation_score = validation.get("valid_percentage", 0)     
-
+
                 # Combined score from string similarity and data validation  
                 combined_score = (similarity * 0.6) + (validation_score * 0.4)
-
+
                 if combined_score > best_score:
                     best_score = combined_score
                     best_match = {
                         "field": field_name,
                         "column": col_name,
                         "match_type": "fuzzy",
-                        "score": best_score,
+                        "score": combined_score,  # Use the combined score   
                         "similarity": similarity,
                         "validation": validation
                     }
-
+
+    # If we have a content match but it wasn't above the high threshold, use it anyway
+    if best_content_match:
+        return best_content_match
+
     # Return best match or empty match if none found
     if best_match:
         return best_match
-
+
     return {
         "field": field_name,
         "column": None,
@@ -416,18 +540,18 @@ def find_best_column_match(field_name: str, field_def: Dict[str, Any],
     }


-def validate_field_values(column_name: str, field_def: Dict[str, Any],       
-                         data: List[Dict[str, Any]],
+def validate_field_values(column_name: str, field_def: Dict[str, Any],       
+                         data: List[Dict[str, Any]],
                          schema: Dict[str, Any]) -> Dict[str, Any]:
     """
     Validate the values in a column against the field definition.
-
+
     Args:
         column_name: Name of the column to validate
         field_def: Field definition from schema
         data: Imported data rows
         schema: Full schema object
-
+
     Returns:
         Dict with validation results
     """
@@ -436,16 +560,16 @@ def validate_field_values(column_name: str, field_def: Dict[str, Any],
     total_count = len(values)
     valid_count = 0
     errors = []
-
+
     # Skip empty values if field is not required
     if not field_def.get("required", False):
         values = [v for v in values if v]
         total_count = len(values)
-
+
     # Validate based on type
     for value in values:
         is_valid = False
-
+
         if not value and not field_def.get("required", False):
             is_valid = True
         elif validate_type == "REGEX":
@@ -469,18 +593,18 @@ def validate_field_values(column_name: str, field_def: Dict[str, Any],
             list_name = field_def.get("list", "")
             list_items = schema.get("lists", {}).get(list_name, [])
             min_distance = field_def.get("distance", 80)
-
+
             # Check against each list item and its aliases
             for item in list_items:
                 item_name = item.get("name", "")
                 item_aliases = item.get("aliases", [])
-
+
                 # Check name
                 similarity = lev.ratio(item_name.lower(), value.lower()) * 100
                 if similarity >= min_distance:
                     is_valid = True
                     break
-
+
                 # Check aliases
                 for alias in item_aliases:
                     similarity = lev.ratio(alias.lower(), value.lower()) * 100
@@ -490,14 +614,14 @@ def validate_field_values(column_name: str, field_def: Dict[str, Any],
         else:
             # If no validation type or unknown type, consider valid
             is_valid = True
-
+
         if is_valid:
             valid_count += 1
         else:
             errors.append(f"Invalid value: {value}")
-
+
     valid_percentage = (valid_count / total_count * 100) if total_count > 0 else 0
-
+
     return {
         "valid": valid_count == total_count,
         "valid_count": valid_count,
@@ -507,42 +631,42 @@ def validate_field_values(column_name: str, field_def: Dict[str, Any],
     }


-def validate_row(row: Dict[str, Any], schema: Dict[str, Any],
+def validate_row(row: Dict[str, Any], schema: Dict[str, Any],
                 field_matches: Dict[str, Dict[str, Any]]) -> Dict[str, Any]: 
     """
     Validate a single data row against the schema.
-
+
     Args:
         row: Data row to validate
         schema: Schema definition
         field_matches: Field to column mapping
-
+
     Returns:
         Dict with row validation results
     """
     schema_fields = schema.get("schema", {})
     valid = True
     field_results = []
-
+
     # Create a reverse mapping from column names to field types
     column_to_field = {}
     for field_name, match_info in field_matches.items():
         column_name = match_info.get("column")
         if column_name:
             column_to_field[column_name] = field_name
-    
+
     # Check for required fields that aren't mapped
     for field_name, field_def in schema_fields.items():
         if not field_def.get("required", False):
             continue
-
+
         # Check if this required field has a mapping
         found = False
         for column, field in column_to_field.items():
             if field == field_name:
                 found = True
                 break
-
+
         if not found:
             # Missing required field
             valid = False
@@ -555,55 +679,80 @@ def validate_row(row: Dict[str, Any], schema: Dict[str, Any],
                 "valid": False,
                 "errors": ["Required field has no matching column"]
             })
-
-    # Validate all mapped columns
-    for column_name, field_name in column_to_field.items():
-        # Get field definition
-        field_def = schema_fields.get(field_name, {})
-
-        # Get value
-        value = row.get(column_name, "")
-
-        # Validate value
-        validation = validate_single_value(value, field_def, schema)
-        field_status = "MATCH" if validation["valid"] else "MISMATCH"        
-
-        field_results.append({
-            "field": field_name,
-            "column": column_name,
-            "value": value,
-            "expected": validation.get("expected"),
-            "status": field_status,
-            "valid": validation["valid"],
-            "errors": validation.get("errors", [])
-        })
-
-        if not validation["valid"]:
-            valid = False
-
+
+    # Process all columns in the row, including unmapped ones
+    for column_name, value in row.items():
+        # Check if this column is mapped to a field
+        if column_name in column_to_field:
+            field_name = column_to_field[column_name]
+
+            # Skip if this is an unmapped field (starts with UNMAPPED_)      
+            if field_name.startswith("UNMAPPED_"):
+                field_results.append({
+                    "field": field_name,
+                    "column": column_name,
+                    "value": value,
+                    "expected": None,
+                    "status": "UNMAPPED",
+                    "valid": True,  # Unmapped columns are considered valid  
+                    "errors": []
+                })
+                continue
+
+            # Get field definition for mapped fields
+            field_def = schema_fields.get(field_name, {})
+
+            # Validate value
+            validation = validate_single_value(value, field_def, schema)     
+            field_status = "MATCH" if validation["valid"] else "MISMATCH"    
+
+            field_results.append({
+                "field": field_name,
+                "column": column_name,
+                "value": value,
+                "expected": validation.get("expected"),
+                "status": field_status,
+                "valid": validation["valid"],
+                "errors": validation.get("errors", [])
+            })
+
+            if not validation["valid"]:
+                valid = False
+        else:
+            # This is an unmapped column
+            field_results.append({
+                "field": f"UNMAPPED_{column_name}",
+                "column": column_name,
+                "value": value,
+                "expected": None,
+                "status": "UNMAPPED",
+                "valid": True,  # Unmapped columns are considered valid      
+                "errors": []
+            })
+
     return {
         "valid": valid,
         "fields": field_results
     }


-def validate_single_value(value: str, field_def: Dict[str, Any],
+def validate_single_value(value: str, field_def: Dict[str, Any],
                          schema: Dict[str, Any]) -> Dict[str, Any]:
     """
     Validate a single value against a field definition.
-
+
     Args:
         value: Value to validate
         field_def: Field definition from schema
         schema: Full schema object
-
+
     Returns:
         Dict with validation result
     """
     validate_type = field_def.get("validate_type", "NONE")
     required = field_def.get("required", False)
     result = {"valid": True}
-
+
     # Check if empty
     if not value:
         if required:
@@ -612,7 +761,7 @@ def validate_single_value(value: str, field_def: Dict[str, Any],
                 "errors": ["Required field is empty"]
             }
         return result
-
+
     # Validate based on type
     if validate_type == "REGEX":
         pattern = field_def.get("regex", ".*")
@@ -622,7 +771,7 @@ def validate_single_value(value: str, field_def: Dict[str, Any],
                 "expected": f"Match pattern {pattern}",
                 "errors": [f"Value does not match pattern: {pattern}"]       
             }
-
+
     elif validate_type == "SA_ID_NUMBER":
         if not validate_sa_id_number(value):
             return {
@@ -630,7 +779,7 @@ def validate_single_value(value: str, field_def: Dict[str, Any],
                 "expected": "Valid SA ID Number",
                 "errors": ["Invalid South African ID number"]
             }
-
+
     elif validate_type == "BANK_ACCOUNT_NUMBER":
         if not validate_bank_account_number(value):
             return {
@@ -638,7 +787,7 @@ def validate_single_value(value: str, field_def: Dict[str, Any],
                 "expected": "Valid bank account number",
                 "errors": ["Invalid bank account number"]
             }
-
+
     elif validate_type == "DECIMAL_AMOUNT":
         if not validate_decimal_amount(value):
             return {
@@ -646,7 +795,7 @@ def validate_single_value(value: str, field_def: Dict[str, Any],
                 "expected": "Decimal amount (e.g. 123.45)",
                 "errors": ["Invalid decimal amount"]
             }
-
+
     elif validate_type == "UNIX_DATE":
         if not validate_date(value):
             return {
@@ -654,7 +803,7 @@ def validate_single_value(value: str, field_def: Dict[str, Any],
                 "expected": "Valid date",
                 "errors": ["Invalid date format"]
             }
-
+
     elif validate_type == "POSTAL_CODE":
         if not validate_postal_code(value):
             return {
@@ -662,7 +811,7 @@ def validate_single_value(value: str, field_def: Dict[str, Any],
                 "expected": "Valid postal code",
                 "errors": ["Invalid postal code"]
             }
-
+
     elif validate_type == "ENUM":
         enum_name = field_def.get("enum", "")
         enum_values = schema.get("enums", {}).get(enum_name, [])
@@ -672,40 +821,40 @@ def validate_single_value(value: str, field_def: Dict[str, Any],
                 "expected": f"One of: {', '.join(enum_values)}",
                 "errors": [f"Value not in allowed list: {enum_name}"]        
             }
-
+
     elif validate_type == "LEV_DISTANCE":
         list_name = field_def.get("list", "")
         list_items = schema.get("lists", {}).get(list_name, [])
         min_distance = field_def.get("distance", 80)
-
+
         # Check against each list item and its aliases
         best_match = None
         best_score = 0
-
+
         for item in list_items:
             item_name = item.get("name", "")
             item_aliases = item.get("aliases", [])
-
+
             # Check name
             similarity = lev.ratio(item_name.lower(), value.lower()) * 100   
             if similarity > best_score:
                 best_score = similarity
                 best_match = item_name
-
+
             # Check aliases
             for alias in item_aliases:
                 similarity = lev.ratio(alias.lower(), value.lower()) * 100   
                 if similarity > best_score:
                     best_score = similarity
                     best_match = item_name
-
+
         if best_score < min_distance:
             return {
                 "valid": False,
                 "expected": f"Match an item in list: {list_name}",
                 "errors": [f"No close match found (best: {best_match}, score: {best_score:.1f}%)"]
             }
-
+
     return result


@@ -715,14 +864,14 @@ def validate_sa_id_number(value: str) -> bool:
     """Validate South African ID number."""
     # Remove any spaces or hyphens
     value = re.sub(r'[\s\-]', '', str(value))
-
+
     # Basic format check: 13 digits
     if not re.match(r'^\d{13}$', value):
         return False
-
+
     # TODO: Implement full SA ID validation logic if needed
     # For now, just check the basic format
-
+
     return True


@@ -730,13 +879,13 @@ def validate_bank_account_number(value: str) -> bool:   
     """Validate bank account number."""
     # Remove any spaces, hyphens, or commas
     value = re.sub(r'[\s\-,]', '', str(value))
-
+
     # Most SA bank accounts are 10-12 digits
     # Some may include special characters like *
     # Allow basic formats for now
     if re.match(r'^[0-9\*]{6,12}$', value):
         return True
-
+
     return False


@@ -744,7 +893,7 @@ def validate_decimal_amount(value: str) -> bool:
     """Validate decimal amount."""
     # Remove currency symbols, spaces, and commas
     value = re.sub(r'[$£€\s,]', '', str(value))
-
+
     # Check if it's a valid decimal number
     try:
         float(value)
@@ -760,14 +909,14 @@ def validate_date(value: str) -> bool:
         '%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%d-%m-%Y', '%m-%d-%Y',
         '%d.%m.%Y', '%m.%d.%Y', '%Y/%m/%d', '%d %b %Y', '%d %B %Y'
     ]
-
+
     for fmt in date_formats:
         try:
             datetime.strptime(str(value), fmt)
             return True
         except ValueError:
             continue
-
+
     # Try Unix timestamp
     try:
         ts = float(value)
@@ -775,7 +924,7 @@ def validate_date(value: str) -> bool:
         return True
     except (ValueError, TypeError, OverflowError):
         pass
-
+
     return False


@@ -783,10 +932,10 @@ def validate_postal_code(value: str) -> bool:
     """Validate postal/zip code."""
     # Remove spaces
     value = re.sub(r'\s', '', str(value))
-
+
     # Different countries have different formats
     # Allow digits, possibly with a hyphen for now
     if re.match(r'^[0-9\-]{4,10}$', value):
         return True
-
-    return False
\ No newline at end of file
+
+    return False
Use code with caution.
